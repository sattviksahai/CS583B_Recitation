{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS4nZ6zetjJay3hYDFJ1TI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "461672132a764bedb9ac88a15591b8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42373c26219a44daa12a767908f151da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6fc35f6d809418da7998386bb714fa3",
              "IPY_MODEL_ecaed77f1b28429d9c669a7a55bb96ce"
            ]
          }
        },
        "42373c26219a44daa12a767908f151da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6fc35f6d809418da7998386bb714fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68e7b0fa9420417c8fc8f22d88866ac1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97ff2ee75c6b4f60b998a99f1a583780"
          }
        },
        "ecaed77f1b28429d9c669a7a55bb96ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02902bc0f603465e8dc1c2525d1304fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [01:58&lt;00:00,  8.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_351cdea0c1204f92a2fd9672f2afdcf9"
          }
        },
        "68e7b0fa9420417c8fc8f22d88866ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97ff2ee75c6b4f60b998a99f1a583780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02902bc0f603465e8dc1c2525d1304fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "351cdea0c1204f92a2fd9672f2afdcf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sattviksahai/CS583B_Recitation/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1-ZUEDbZW7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "74e0f893-d3a3-4a92-9058-ca2d1a11f3e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJUY273KaZih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "30b355e0-9be1-4a4a-dc39-055ca1fb7080"
      },
      "source": [
        "!unzip drive/My\\ Drive/data/fra-eng.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/data/fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jz47YTRaQnT",
        "colab_type": "text"
      },
      "source": [
        "1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7PHXXlZW1Co",
        "colab_type": "text"
      },
      "source": [
        "1.1. Load and clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkf0_Q6FWvy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        "\n",
        "def clean_data(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return numpy.array(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEkUFoHQXPq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "072e75fd-0da5-4c78-bde6-10fd98a492c5"
      },
      "source": [
        "filename = 'fra.txt'\\\n",
        "\n",
        "# e.g., n_train = 20000\n",
        "train_val_percentage = 0.9\n",
        "train_val_split = 0.8\n",
        "test_percentage = 1 - train_val_percentage\n",
        "print(\"Train val / Test split is: \",train_val_percentage, \" and \", test_percentage)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train val / Test split is:  0.9  and  0.09999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh2ImTu6XVWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# French dataset\n",
        "# load dataset\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# split into Language1-Language2 pairs\n",
        "pairs = to_pairs(doc)\n",
        "\n",
        "# clean sentences\n",
        "clean_pairs = clean_data(pairs)[0:int(len(pairs)*train_val_percentage), :]\n",
        "clean_pairs_test = clean_data(pairs)[int(len(pairs)*train_val_percentage):, :]\n",
        "test_indices = list(range(len(clean_pairs_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x5lhDGkanrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7be41053-1246-4b98-a03a-6ccd2790f42d"
      },
      "source": [
        "for i in range(3000, 3010):\n",
        "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[were smart] => [nous sommes intelligentes]\n",
            "[were sorry] => [nous sommes desoles]\n",
            "[were sorry] => [nous sommes desolees]\n",
            "[were stuck] => [nous sommes coinces]\n",
            "[were stuck] => [nous sommes coincees]\n",
            "[were tired] => [nous sommes fatigues]\n",
            "[were tired] => [nous sommes fatiguees]\n",
            "[were twins] => [nous sommes jumeaux]\n",
            "[were twins] => [nous sommes jumelles]\n",
            "[what a bore] => [quel emmerdeur]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4XTK66FauLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab6305cd-3922-410d-8fe3-8cf05a9c864a"
      },
      "source": [
        "train_val_indices = list(range(len(clean_pairs)))\n",
        "numpy.random.shuffle(train_val_indices)\n",
        "train_indices = train_val_indices[:int(len(clean_pairs)*train_val_split)]\n",
        "val_indices = train_val_indices[int(len(clean_pairs)*train_val_split):]\n",
        "\n",
        "print(len(train_val_indices), len(train_indices), len(val_indices))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158060 126448 31612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK6t_jUXa329",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e595d8ca-cf75-45e5-8ea0-933306648349"
      },
      "source": [
        "input_texts = clean_pairs[:, 0]\n",
        "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
        "\n",
        "print('Length of input_texts:  ' + str(input_texts.shape))\n",
        "print('Length of French target_texts: ' + str(len(target_texts)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input_texts:  (158060,)\n",
            "Length of French target_texts: 158060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GxCpFxfa-cN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32867349-be7a-4e87-9d77-c038994ce967"
      },
      "source": [
        "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
        "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
        "\n",
        "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
        "print('max length of French target sentences: %d' % (max_decoder_seq_length))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length of input  sentences: 45\n",
            "max length of French target sentences: 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-YbyviubcUs",
        "colab_type": "text"
      },
      "source": [
        "2. Text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bmV6HOAbe4E",
        "colab_type": "text"
      },
      "source": [
        "2.1. Convert texts to sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ5IrVodbYPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4e8b52ed-cc90-47ec-e39e-756f377c97a9"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# encode and pad sequences\n",
        "def text2sequences(max_len, lines):\n",
        "    tokenizer = Tokenizer(char_level=True, filters='')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    seqs = tokenizer.texts_to_sequences(lines)\n",
        "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
        "    return seqs_pad, tokenizer.word_index\n",
        "\n",
        "\n",
        "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
        "                                                      input_texts)\n",
        "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
        "                                                       target_texts)\n",
        "\n",
        "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
        "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
        "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
        "print('shape of target_token_index: ' + str(len(target_token_index)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of encoder_input_seq: (158060, 45)\n",
            "shape of input_token_index: 27\n",
            "shape of decoder_input_seq: (158060, 95)\n",
            "shape of target_token_index: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMoyTASFbhf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67dd76c9-12d8-45b4-d1f2-b55c136be7ed"
      },
      "source": [
        "num_encoder_tokens = len(input_token_index) + 1\n",
        "num_decoder_tokens = len(target_token_index) + 1\n",
        "\n",
        "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
        "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_encoder_tokens: 28\n",
            "num_decoder_tokens: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "854rYSMBcGzz",
        "colab_type": "text"
      },
      "source": [
        "2.2. One-hot encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACBfFf5ncCLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# one hot encode target sequence\n",
        "def onehot_encode(sequences, max_len, vocab_size):\n",
        "    n = len(sequences)\n",
        "    data = numpy.zeros((n, max_len, vocab_size))\n",
        "    for i in range(n):\n",
        "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH2R2PU-cJOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, indices, encoder_input_seq, decoder_input_seq, num_encoder_tokens, \n",
        "                 num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length,\n",
        "                 batch_size=512, shuffle=True):\n",
        "        \n",
        "        self.encoder_input_seq = encoder_input_seq\n",
        "        self.decoder_input_seq = decoder_input_seq\n",
        "        self.num_encoder_tokens = num_encoder_tokens\n",
        "        self.num_decoder_tokens = num_decoder_tokens\n",
        "        self.max_encoder_seq_length = max_encoder_seq_length\n",
        "        self.max_decoder_seq_length = max_decoder_seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = indices\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        #print(self.list_IDs)\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.list_IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(indexes)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.list_IDs)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "\n",
        "        batch_encoder_input_seq = []\n",
        "        batch_decoder_input_seq = []\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            batch_encoder_input_seq.append(self.encoder_input_seq[ID])\n",
        "            batch_decoder_input_seq.append(self.decoder_input_seq[ID])\n",
        "            \n",
        "        encoder_input_data = onehot_encode(numpy.stack(batch_encoder_input_seq), self.max_encoder_seq_length, self.num_encoder_tokens)\n",
        "        decoder_input_data = onehot_encode(numpy.stack(batch_decoder_input_seq), self.max_decoder_seq_length, self.num_decoder_tokens)\n",
        "\n",
        "        decoder_target_seq = numpy.zeros(numpy.stack(batch_decoder_input_seq).shape)\n",
        "        decoder_target_seq[:, 0:-1] = numpy.stack(batch_decoder_input_seq)[:, 1:]\n",
        "        decoder_target_data = onehot_encode(decoder_target_seq, \n",
        "                                            self.max_decoder_seq_length, \n",
        "                                            self.num_decoder_tokens)\n",
        "\n",
        "        return [encoder_input_data,decoder_input_data], decoder_target_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6ytAhEcgD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = DataGenerator(train_indices, encoder_input_seq, decoder_input_seq, num_encoder_tokens, \n",
        "                 num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length)\n",
        "\n",
        "val_generator = DataGenerator(val_indices, encoder_input_seq, decoder_input_seq, num_encoder_tokens, \n",
        "                 num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAJ4Vdqcnwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0b33322e-1ae7-4d31-f5e8-b93e2da2eb93"
      },
      "source": [
        "# Data Loader sanity check\n",
        "for i, (data, label) in enumerate(train_generator):\n",
        "    if i>10:\n",
        "        break\n",
        "    print(data[0].shape, data[1].shape, label.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n",
            "(512, 45, 28) (512, 95, 30) (512, 95, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObMJz4bOcymj",
        "colab_type": "text"
      },
      "source": [
        "3. Build the networks (for training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UECWdQczmg",
        "colab_type": "text"
      },
      "source": [
        "3.1. Encoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAYU3WXHcp5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, LSTM, Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 128\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
        "                       name='encoder_inputs')\n",
        "\n",
        "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
        "                                  dropout=0.4, name='encoder_lstm'))\n",
        "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# build the encoder network model\n",
        "encoder_model = Model(inputs=encoder_inputs, \n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbBDdN9hc6_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ae5db217-adf7-4a54-b428-0d03495cea54"
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=encoder_model, show_shapes=False,\n",
        "    to_file='encoder.pdf'\n",
        ")\n",
        "\n",
        "encoder_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     (None, None, 28)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) [(None, 256), (None, 160768      encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional_1[0][1]            \n",
            "                                                                 bidirectional_1[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256)          0           bidirectional_1[0][2]            \n",
            "                                                                 bidirectional_1[0][4]            \n",
            "==================================================================================================\n",
            "Total params: 160,768\n",
            "Trainable params: 160,768\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS0AEOGPdCIY",
        "colab_type": "text"
      },
      "source": [
        "3.2. Decoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK1MGKc9c9Rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# French\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(2*latent_dim,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(2*latent_dim,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# set the LSTM layer\n",
        "decoder_lstm = LSTM(2*latent_dim, return_sequences=True, \n",
        "                    return_state=True, dropout=0.4, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# set the dense layer\n",
        "#decoder_lstm_outputs = Dropout(0.2)(decoder_lstm_outputs)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs, state_h, state_c],\n",
        "                      name='decoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UDN4jXWdF9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "91aca49e-2e34-4424-ce3b-714c26387f7c"
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=decoder_model, show_shapes=False,\n",
        "    to_file='decoder.pdf'\n",
        ")\n",
        "\n",
        "decoder_model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input_h (InputLayer)    (None, 256)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input_c (InputLayer)    (None, 256)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
            "                                                                 decoder_input_h[0][0]            \n",
            "                                                                 decoder_input_c[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 301,598\n",
            "Trainable params: 301,598\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_memrI6dLHa",
        "colab_type": "text"
      },
      "source": [
        "3.3. Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpZ4CIssdISL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
        "              outputs=[decoder_pred], \n",
        "              name='model_training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40kAme7NdRZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21d19ccd-d494-4c4d-a1a5-832c7e2abd30"
      },
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"decoder_lstm/while:4\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"decoder_input_h:0\", shape=(None, 256), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtNrGISJdTFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a66abb94-aa0a-4270-b8bd-6a31e239450d"
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=model, show_shapes=False,\n",
        "    to_file='model_training.pdf'\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Model)                 [(None, 256), (None, 160768      encoder_input_x[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
            "                                                                 encoder[1][0]                    \n",
            "                                                                 encoder[1][1]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[1][0]               \n",
            "==================================================================================================\n",
            "Total params: 462,366\n",
            "Trainable params: 462,366\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyzemo6HdVD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "73840b0b-8134-440f-9e54-7f6c7d103dca"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.002), loss=['categorical_crossentropy'])\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=4,\n",
        "                    epochs = 1)\n",
        "\n",
        "model.save('seq2seq.h5')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "245/246 [============================>.] - ETA: 3s - loss: 0.9452\n",
            "\n",
            "246/246 [==============================] - 1048s 4s/step - loss: 0.9446 - val_loss: 0.6748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et37xV9idlS2",
        "colab_type": "text"
      },
      "source": [
        "4. Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdnGDjpvdY8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-6-brC4dpS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # this line of code is greedy selection\n",
        "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "        \n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cN7-2HGeyj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63387f17-319a-44f6-9d43-824b69b46dba"
      },
      "source": [
        "for seq_index in range(70, 90):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = train_generator[seq_index]\n",
        "    decoded_sentence = decode_sequence(numpy.expand_dims(input_seq[0][0][0], axis=0))\n",
        "    print('-')\n",
        "    print('English:       ', input_texts[seq_index])\n",
        "    print('French (true): ', target_texts[seq_index][1:-1])\n",
        "    print('French (pred): ', decoded_sentence[0:-1])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "English:        we try\n",
            "French (true):  on essaye\n",
            "French (pred):  il e tous ant ant ant ant ant ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais e\n",
            "-\n",
            "English:        we won\n",
            "French (true):  nous avons gagne\n",
            "French (pred):  je ne sous pas aus pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        we won\n",
            "French (true):  nous gagnames\n",
            "French (pred):  je ne sous pas ais pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        we won\n",
            "French (true):  nous lavons emporte\n",
            "French (pred):  tu s sous pas ent pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais e\n",
            "-\n",
            "English:        we won\n",
            "French (true):  nous lemportames\n",
            "French (pred):  tom ent ant ant ant ant ant ant ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais\n",
            "-\n",
            "English:        ask tom\n",
            "French (true):  demande a tom\n",
            "French (pred):  je ne sous pas aus pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        awesome\n",
            "French (true):  fantastique\n",
            "French (pred):  elle tous pas ent pas ent ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent a\n",
            "-\n",
            "English:        be calm\n",
            "French (true):  sois calme\n",
            "French (pred):  ce se tous ant ant ant en ere te te te te te te te te te te te te te te te te te te te te te te\n",
            "-\n",
            "English:        be calm\n",
            "French (true):  soyez calme\n",
            "French (pred):  nous aus pas ent pas ent ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ai\n",
            "-\n",
            "English:        be calm\n",
            "French (true):  soyez calmes\n",
            "French (pred):  pe se sous pas ent pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        be cool\n",
            "French (true):  sois detendu\n",
            "French (pred):  tom ent pas ent pas ent ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais\n",
            "-\n",
            "English:        be fair\n",
            "French (true):  sois juste\n",
            "French (pred):  tom ent ant ant ant ant ant ant ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais\n",
            "-\n",
            "English:        be fair\n",
            "French (true):  soyez juste\n",
            "French (pred):  tu s sous pas ent pas ent ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent a\n",
            "-\n",
            "English:        be fair\n",
            "French (true):  soyez justes\n",
            "French (pred):  je ne sous pas aus pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        be fair\n",
            "French (true):  sois equitable\n",
            "French (pred):  ce se tous pas ent pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        be fair\n",
            "French (true):  soyez equitable\n",
            "French (pred):  je ne sous pas aus pas ent ait en ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais e\n",
            "-\n",
            "English:        be fair\n",
            "French (true):  soyez equitables\n",
            "French (pred):  ll s tous ant ant ant ant ant ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais e\n",
            "-\n",
            "English:        be kind\n",
            "French (true):  sois gentil\n",
            "French (pred):  je ne sous pas ais pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais \n",
            "-\n",
            "English:        be nice\n",
            "French (true):  sois gentil\n",
            "French (pred):  tom ent pas ent pas ent ant ant ant ant ent ais ent ais ent ais ent ais ent ais ent ais ent ais\n",
            "-\n",
            "English:        be nice\n",
            "French (true):  sois gentille\n",
            "French (pred):  tu s sous pas ent pas ent ait ent ais ent ais ent ais ent ais ent ais ent ais ent ais ent ais e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YYHZq6Le8mr",
        "colab_type": "text"
      },
      "source": [
        "5. Evaluate the translation using BLEU score\n",
        "\n",
        "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUamwuque8__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "461672132a764bedb9ac88a15591b8e4",
            "42373c26219a44daa12a767908f151da",
            "d6fc35f6d809418da7998386bb714fa3",
            "ecaed77f1b28429d9c669a7a55bb96ce",
            "68e7b0fa9420417c8fc8f22d88866ac1",
            "97ff2ee75c6b4f60b998a99f1a583780",
            "02902bc0f603465e8dc1c2525d1304fb",
            "351cdea0c1204f92a2fd9672f2afdcf9"
          ]
        },
        "outputId": "5d0b816c-58ac-4d2a-df23-76f29917365a"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def find_bleu_score(translation, reference):\n",
        "    # split to words\n",
        "    translation_w = str(translation).split()\n",
        "    reference_w = str(reference).split()\n",
        "#     print(\"translation:\",translation_w)\n",
        "#     print(\"reference\",reference_w)\n",
        "    score = sentence_bleu([reference_w],translation_w)\n",
        "    return score\n",
        "avg_score = 0\n",
        "for i, (test_seq) in tqdm(enumerate(clean_pairs_test[:1000,:]), total=1000):\n",
        "    input_seq = pad_sequences(numpy.array([input_token_index[x] for x in test_seq[0] ]).reshape(1,len(test_seq[0])), maxlen=max_encoder_seq_length, padding='post')\n",
        "    input_onehot = onehot_encode(input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
        "    translation = decode_sequence(input_onehot)\n",
        "    score = find_bleu_score(test_seq[1], translation)\n",
        "    avg_score = ((avg_score*i)/(i+1)) + score/(i+1)\n",
        "#     print(\"avg score: \",avg_score, \" current score: \",score, '\\n')\n",
        "print(\"avg score: \",avg_score)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "461672132a764bedb9ac88a15591b8e4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "avg score:  0.05784839902902429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8igtpJTMmC6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}